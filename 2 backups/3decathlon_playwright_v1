import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import csv

OUTPUT_FILE = "decathlon_output.csv"
BASE_URL = "https://www.decathlon.sg/c/men/shoes.html"

async def main():
    items = []
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()

        print(f"üåê Visiting: {BASE_URL}")
        await page.goto(BASE_URL)
        await page.wait_for_timeout(5000)

        # Keep clicking "View More" if visible
        max_clicks = 10
        click_count = 0

        while click_count < max_clicks:
            try:
                show_more_button = await page.query_selector('button[data-cy="Show-More"]')
                if show_more_button:
                    print(f"üîÅ Clicking 'View More' ({click_count + 1}/{max_clicks})...")
                    await show_more_button.click()
                    click_count += 1
                    await page.wait_for_timeout(3000)
                else:
                    print("‚úÖ 'View More' button no longer visible.")
                    break
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to click 'View More': {e}")
                break

        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")

        product_cards = soup.select('a[href^="/p/"]')
        seen = set()

        for card in product_cards:
            href = card.get("href")
            if not href or href in seen:
                continue
            seen.add(href)
            product_url = "https://www.decathlon.sg" + href

            # Title
            title_div = card.select_one('div[title]')
            title = title_div.get("title", "").strip() if title_div else ""

            # Price
            price_span = card.select_one('span.vp-price-amount')
            price = price_span.text.strip() if price_span else ""

            # Image
            img_tag = card.select_one('img')
            image_url = img_tag.get("src", "").strip() if img_tag else ""

            if title:
                items.append([title, price, image_url, product_url])


        await browser.close()

    with open(OUTPUT_FILE, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Title", "Price", "ImageURL", "ProductURL"])
        writer.writerows(items)

    print(f"\n‚úÖ Done. Scraped {len(items)} Decathlon products to {OUTPUT_FILE}")


if __name__ == "__main__":
    asyncio.run(main())


"""
only 12 prducts are scraped, so we need to click "View More" button until it is no longer visible.
But clicking view more, only 12 products are loaded each time, which means this usually happens because the DOM selectors are pulling from non-product or duplicate elements, or not all products load cleanly into the page even after clicks.

"""